import streamlit as st
import pandas as pd
import io
import re
import smtplib
import gspread
from datetime import date
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

# ==========================================
# ğŸ” 1. å®‰å…¨è¨­å®šèˆ‡ç’°å¢ƒé…ç½® (Secrets)
# ==========================================
st.set_page_config(page_title="äº¤é€šäº‹æ•…çµ±è¨ˆç³»çµ±", layout="wide", page_icon="ğŸš‘")

try:
    MY_EMAIL = st.secrets["email"]["user"]
    MY_PASSWORD = st.secrets["email"]["password"]
    GCP_CREDS = st.secrets["gcp_service_account"]
except Exception as e:
    st.error("âŒ æ‰¾ä¸åˆ° Secrets è¨­å®šï¼è«‹åœ¨ .streamlit/secrets.toml ä¸­é…ç½® [email] èˆ‡ [gcp_service_account]ã€‚")
    st.stop()

# æŒ‡å®šçš„é›²ç«¯è©¦ç®—è¡¨ç¶²å€èˆ‡è¨­å®š
GOOGLE_SHEET_URL = "https://docs.google.com/spreadsheets/d/1HaFu5PZkFDUg7WZGV9khyQ0itdGXhXUakP4_BClFTUg/edit"
SMTP_SERVER = "smtp.gmail.com"
SMTP_PORT = 587
TO_EMAIL = MY_EMAIL 

# ==========================================
# ğŸ› ï¸ 2. å·¥å…·å‡½å¼ (Parsing & Formatting)
# ==========================================

def parse_raw(f):
    """è§£æä¸Šå‚³æª”æ¡ˆ (æ”¯æ´ CSV èˆ‡ Excel)"""
    try:
        f.seek(0)
        if f.name.endswith('.csv'):
            return pd.read_csv(f, header=None)
        else:
            return pd.read_excel(f, header=None)
    except Exception as e:
        st.error(f"æª”æ¡ˆ {f.name} è®€å–å¤±æ•—: {e}")
        return None

def clean_data(df_raw):
    """æ¸…æ´—å ±è¡¨ï¼Œæå–é—œéµæ¬„ä½"""
    df_raw[0] = df_raw[0].astype(str)
    df_data = df_raw[df_raw[0].str.contains("æ‰€|ç¸½è¨ˆ|åˆè¨ˆ", na=False)].copy()
    cols = {0: "Station", 5: "A1_Deaths", 9: "A2_Injuries"}
    df_data = df_data.rename(columns=cols)
    
    # æ•¸å€¼æ¸…ç† (è™•ç†åƒåˆ†ä½é€—è™Ÿ)
    for c in [5, 9]:
        target = cols[c]
        df_data[target] = pd.to_numeric(df_data[target].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
    
    df_data['Station_Short'] = df_data['Station'].str.replace('æ´¾å‡ºæ‰€', 'æ‰€').str.replace('ç¸½è¨ˆ', 'åˆè¨ˆ')
    return df_data

def get_gsheet_rich_text_req(sheet_id, row_idx, col_idx, text):
    """ç”¢è£½ Google Sheets ç´…å­—æ ¼å¼è«‹æ±‚"""
    text = str(text)
    tokens = re.split(r'([0-9\(\)\/\-\.\%]+)', text)
    runs = []
    current_pos = 0
    for token in tokens:
        if not token: continue
        color = {"red": 1, "green": 0, "blue": 0} if re.match(r'^[0-9\(\)\/\-\.\%]+$', token) else {"red": 0, "green": 0, "blue": 0}
        runs.append({"startIndex": current_pos, "format": {"foregroundColor": color, "bold": True}})
        current_pos += len(token)
    return {
        "updateCells": {
            "rows": [{"values": [{"userEnteredValue": {"stringValue": text}, "textFormatRuns": runs}]}],
            "fields": "userEnteredValue,textFormatRuns",
            "range": {"sheetId": sheet_id, "startRowIndex": row_idx, "endRowIndex": row_idx + 1, "startColumnIndex": col_idx, "endColumnIndex": col_idx + 1}
        }
    }

# ==========================================
# ğŸ“Š 3. æ ¸å¿ƒè¨ˆç®—é‚è¼¯ (ä¿®æ­£æ’åºèˆ‡å°é½Š)
# ==========================================

def build_a1_table(wk_df, cur_df, lst_df, stations, date_labels):
    col = 'A1_Deaths'
    m = pd.merge(wk_df[['Station_Short', col]], cur_df[['Station_Short', col]], on='Station_Short', suffixes=('_wk', '_cur'))
    m = pd.merge(m, lst_df[['Station_Short', col]], on='Station_Short').rename(columns={col: col+'_lst'})
    
    # ğŸŒŸ åƒ…ä¿ç•™æŒ‡å®šå–®ä½ä¸¦è‡ªå®šç¾©æ’åº
    m = m[m['Station_Short'].isin(stations)].copy()
    m['Station_Short'] = pd.Categorical(m['Station_Short'], categories=stations, ordered=True)
    m = m.sort_values('Station_Short')
    
    # è¨ˆç®—åˆè¨ˆä¸¦ç½®é ‚
    total = m.select_dtypes(include='number').sum().to_dict()
    total['Station_Short'] = 'åˆè¨ˆ'
    m = pd.concat([pd.DataFrame([total]), m], ignore_index=True)
    
    m['Diff'] = m[col+'_cur'] - m[col+'_lst']
    m = m[['Station_Short', col+'_wk', col+'_cur', col+'_lst', 'Diff']]
    m.columns = ['çµ±è¨ˆæœŸé–“', f'æœ¬æœŸ({date_labels["wk"]})', f'æœ¬å¹´ç´¯è¨ˆ({date_labels["cur"]})', f'å»å¹´ç´¯è¨ˆ({date_labels["lst"]})', 'æ¯”è¼ƒ']
    return m

def build_a2_table(wk_df, cur_df, lst_df, stations, date_labels):
    col = 'A2_Injuries'
    m = pd.merge(wk_df[['Station_Short', col]], cur_df[['Station_Short', col]], on='Station_Short', suffixes=('_wk', '_cur'))
    m = pd.merge(m, lst_df[['Station_Short', col]], on='Station_Short').rename(columns={col: col+'_lst'})
    
    # ğŸŒŸ åƒ…ä¿ç•™æŒ‡å®šå–®ä½ä¸¦è‡ªå®šç¾©æ’åº
    m = m[m['Station_Short'].isin(stations)].copy()
    m['Station_Short'] = pd.Categorical(m['Station_Short'], categories=stations, ordered=True)
    m = m.sort_values('Station_Short')
    
    # è¨ˆç®—åˆè¨ˆä¸¦ç½®é ‚
    total = m.select_dtypes(include='number').sum().to_dict()
    total['Station_Short'] = 'åˆè¨ˆ'
    m = pd.concat([pd.DataFrame([total]), m], ignore_index=True)
    
    m['Diff'] = m[col+'_cur'] - m[col+'_lst']
    m['Pct'] = m.apply(lambda x: f"{(x['Diff']/x[col+'_lst']):.2%}" if x[col+'_lst'] != 0 else "0.00%", axis=1)
    
    # 7 æ¬„ä½æ’åˆ—: ç²¾æº–æ’å…¥ 'Prev'
    m.insert(2, 'Prev', '-')
    m = m[['Station_Short', col+'_wk', 'Prev', col+'_cur', col+'_lst', 'Diff', 'Pct']]
    m.columns = ['çµ±è¨ˆæœŸé–“', f'æœ¬æœŸ({date_labels["wk"]})', 'å‰æœŸ', f'æœ¬å¹´ç´¯è¨ˆ({date_labels["cur"]})', f'å»å¹´ç´¯è¨ˆ({date_labels["lst"]})', 'æ¯”è¼ƒ', 'å¢æ¸›æ¯”ä¾‹']
    return m

# ==========================================
# â˜ï¸ 4. é›²ç«¯åŒæ­¥
# ==========================================

def sync_to_gsheet(df_a1, df_a2):
    try:
        gc = gspread.service_account_from_dict(GCP_CREDS)
        sh = gc.open_by_url(GOOGLE_SHEET_URL)
        
        def update_ws(ws_index, df, title_text):
            ws = sh.get_worksheet(ws_index)
            ws.batch_clear(["A3:Z100"]) 
            ws.update_acell('A1', title_text)
            data = [[int(x) if isinstance(x, (int, float)) and not isinstance(x, bool) else x for x in row] for row in df.values.tolist()]
            ws.update('A3', data)
            
            # å¥—ç”¨ Rich Text (ç´…å­—æ•¸å­—)
            reqs = [get_gsheet_rich_text_req(ws.id, 1, i, col) for i, col in enumerate(df.columns)]
            sh.batch_update({"requests": reqs})
        
        update_ws(2, df_a1, "A1é¡äº¤é€šäº‹æ•…æ­»äº¡äººæ•¸çµ±è¨ˆè¡¨") # ç¬¬3å€‹åˆ†é 
        update_ws(3, df_a2, "A2é¡äº¤é€šäº‹æ•…å—å‚·äººæ•¸çµ±è¨ˆè¡¨") # ç¬¬4å€‹åˆ†é 
        return True, "âœ… é›²ç«¯è©¦ç®—è¡¨åŒæ­¥æˆåŠŸ"
    except Exception as e:
        return False, f"âŒ è©¦ç®—è¡¨åŒæ­¥å¤±æ•—: {e}"

# ==========================================
# ğŸš€ 5. ä¸»ç¨‹å¼æµç¨‹
# ==========================================

st.title("ğŸš‘ äº¤é€šäº‹æ•…çµ±è¨ˆç³»çµ± (å–®ä½æ’åºä¿®æ­£ç‰ˆ)")

uploaded_files = st.file_uploader("è«‹åŒæ™‚ä¸Šå‚³ 3 å€‹å ±è¡¨æª”æ¡ˆ", accept_multiple_files=True)

if not uploaded_files:
    st.info("ğŸ’¡ æç¤ºï¼šè«‹ä¸€æ¬¡é¸æ“‡ä¸‰å€‹å ±è¡¨æª”æ¡ˆ (CSV/Excel) ä¸Šå‚³ã€‚")
elif len(uploaded_files) != 3:
    st.warning(f"ç›®å‰å·²ä¸Šå‚³ {len(uploaded_files)} å€‹æª”æ¡ˆï¼Œé‚„å·® {3-len(uploaded_files)} å€‹æ‰èƒ½å•Ÿå‹•ã€‚")
else:
    with st.status("æ­£åœ¨è™•ç†æ•¸æ“šèˆ‡åŒæ­¥...", expanded=True) as status:
        try:
            files_meta = []
            for f in uploaded_files:
                df_raw = parse_raw(f)
                if df_raw is None: continue
                
                # åµæ¸¬æ¨™é¡Œä¸­çš„æ—¥æœŸ (æ°‘åœ‹å¹´)
                dates = re.findall(r'(\d{3})[./](\d{1,2})[./](\d{1,2})', str(df_raw.iloc[:5, :5].values))
                if len(dates) >= 2:
                    d_range = f"{int(dates[0][1]):02d}{int(dates[0][2]):02d}-{int(dates[1][1]):02d}{int(dates[1][2]):02d}"
                    files_meta.append({
                        'df': clean_data(df_raw),
                        'year': int(dates[1][0]),
                        'date_range': d_range,
                        'is_cumu': (int(dates[0][1]) == 1)
                    })
            
            if len(files_meta) < 3:
                st.error("âŒ æª”æ¡ˆæ—¥æœŸè¾¨è­˜å¤±æ•—ã€‚è«‹ç¢ºèªå ±è¡¨é¦–å¹¾åˆ—åŒ…å«æ°‘åœ‹å¹´æœˆã€‚")
                st.stop()

            # æ’åºèˆ‡åˆ†é…
            files_meta.sort(key=lambda x: x['year'], reverse=True)
            cur_year = files_meta[0]['year']
            
            df_wk = [f for f in files_meta if f['year'] == cur_year and not f['is_cumu']][0]
            df_cur = [f for f in files_meta if f['year'] == cur_year and f['is_cumu']][0]
            df_lst = [f for f in files_meta if f['year'] < cur_year][0]
            
            date_labels = {"wk": df_wk['date_range'], "cur": df_cur['date_range'], "lst": df_lst['date_range']}
            
            # ğŸŒŸ æŒ‡å®šé †åº
            stations_order = ['è–äº­æ‰€', 'é¾æ½­æ‰€', 'ä¸­èˆˆæ‰€', 'çŸ³é–€æ‰€', 'é«˜å¹³æ‰€', 'ä¸‰å’Œæ‰€']

            # è¨ˆç®—è¡¨æ ¼
            a1_res = build_a1_table(df_wk['df'], df_cur['df'], df_lst['df'], stations_order, date_labels)
            a2_res = build_a2_table(df_wk['df'], df_cur['df'], df_lst['df'], stations_order, date_labels)

            # åŒæ­¥é›²ç«¯
            gs_ok, gs_msg = sync_to_gsheet(a1_res, a2_res)
            st.write(gs_msg)
            
            status.update(label="âœ… è™•ç†å®Œæˆï¼", state="complete", expanded=False)
            
            st.success("çµ±è¨ˆæ•¸æ“šå·²æ›´æ–°ï¼Œé †åºï¼šåˆè¨ˆ > è–äº­ > é¾æ½­ > ä¸­èˆˆ > çŸ³é–€ > é«˜å¹³ > ä¸‰å’Œ")
            st.dataframe(a1_res, use_container_width=True)
            st.dataframe(a2_res, use_container_width=True)

        except Exception as e:
            st.error(f"åˆ†æç™¼ç”ŸéŒ¯èª¤: {e}")
