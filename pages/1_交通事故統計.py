import pandas as pd
import io
import re
from google.colab import files
from google.colab import auth
import gspread
from google.auth import default
from datetime import datetime

def analyze_traffic_stats_gsheet_smart():
    print("ğŸš€ è«‹ä¸Šå‚³ä¸‰å€‹æª”æ¡ˆï¼ˆæœ¬é€±ã€ä»Šå¹´ç´¯è¨ˆã€å»å¹´ç´¯è¨ˆï¼‰ï¼Œé †åºèˆ‡æª”åä¸æ‹˜...")
    uploaded = files.upload()
    
    if len(uploaded) < 3:
        print("âš ï¸ è­¦å‘Šï¼šæª”æ¡ˆæ•¸é‡ä¸è¶³ 3 å€‹ï¼Œå¯èƒ½æœƒå°è‡´è¨ˆç®—éŒ¯èª¤ã€‚")

    # --- 1. å®šç¾©è§£æå‡½å¼ (å…ˆè®€å–æ‰€æœ‰æª”æ¡ˆ) ---
    def parse_police_stats_raw(file_obj):
        try:
            df_raw = pd.read_csv(file_obj, header=None)
        except:
            file_obj.seek(0)
            df_raw = pd.read_excel(file_obj, header=None)
        return df_raw

    # --- 2. æ™ºæ…§è¾¨è­˜æª”æ¡ˆèº«åˆ† ---
    file_data_map = {} # æš«å­˜è§£æå¾Œçš„è³‡æ–™
    
    print("ğŸ” æ­£åœ¨åˆ†ææª”æ¡ˆå…§å®¹ä»¥è‡ªå‹•åˆ†é¡...")
    
    for filename, content in uploaded.items():
        file_obj = io.BytesIO(content)
        df = parse_police_stats_raw(file_obj)
        
        # æŠ“å–æ—¥æœŸå­—ä¸²ï¼Œä¾‹å¦‚ "çµ±è¨ˆæ—¥æœŸï¼š114/11/21 è‡³ 114/11/27"
        try:
            date_str = df.iloc[1, 0].replace("çµ±è¨ˆæ—¥æœŸï¼š", "").strip()
            # ç°¡å–®æ­£å‰‡æŠ“å–å¹´ä»½èˆ‡æœˆä»½
            dates = re.findall(r'(\d{3})/(\d{2})/(\d{2})', date_str)
            # dates çµæ§‹: [('114', '11', '21'), ('114', '11', '27')]
            
            if not dates:
                print(f"âš ï¸ ç„¡æ³•è­˜åˆ¥æ—¥æœŸï¼š{filename}")
                continue
                
            start_y, start_m, start_d = map(int, dates[0])
            end_y, end_m, end_d = map(int, dates[1])
            
            # åˆ¤æ–·é‚è¼¯
            # 1. æœŸé–“å¾ˆçŸ­ (å°æ–¼ 30 å¤©) -> æœ¬æœŸ (Weekly)
            # 2. æœŸé–“å¾ˆé•· + å¹´ä»½è¼ƒå¤§ -> ä»Šå¹´ç´¯è¨ˆ (Current)
            # 3. æœŸé–“å¾ˆé•· + å¹´ä»½è¼ƒå° -> å»å¹´ç´¯è¨ˆ (Last)
            
            # é€™è£¡ç”¨ç°¡æ˜“åˆ¤æ–·ï¼šè‹¥èµ·å§‹æœˆæ˜¯ 1æœˆ ä¸” çµæŸæœˆå¤§æ–¼ 1æœˆï¼Œé€šå¸¸æ˜¯ç´¯è¨ˆ
            is_cumulative = (start_m == 1 and end_m >= 1)
            
            # æˆ–æ˜¯è¨ˆç®—å¤©æ•¸å·®ç•° (ç•¥éè¤‡é›œ datetimeï¼Œç›´æ¥çœ‹æœˆä»½è·¨åº¦)
            month_diff = (end_y - start_y) * 12 + (end_m - start_m)
            
            if month_diff == 0 and (end_d - start_d) < 20:
                category = 'weekly'
            else:
                # ç´¯è¨ˆæª”ï¼Œæ¯”è¼ƒå¹´ä»½
                # é€™è£¡å…ˆå­˜èµ·ä¾†ï¼Œç­‰ç­‰æ¯”è¼ƒå“ªä¸€å€‹å¹´ä»½å¤§
                category = f'cumulative_{start_y}'
            
            file_data_map[filename] = {
                'df': df,
                'date_str': date_str,
                'category': category,
                'year': start_y
            }
        except Exception as e:
            print(f"âš ï¸ æª”æ¡ˆè§£æå¤±æ•— {filename}: {e}")

    # åˆ†é…è§’è‰²
    df_wk = None
    df_cur = None
    df_lst = None
    d_wk = ""
    d_cur = ""
    d_lst = ""

    # æ‰¾å‡º Weekly
    for fname, data in file_data_map.items():
        if data['category'] == 'weekly':
            df_wk = data['df']
            d_wk = data['date_str']
            break
            
    # æ‰¾å‡º Current å’Œ Last (æ¯”è¼ƒå¹´ä»½)
    cumulative_files = [d for d in file_data_map.values() if 'cumulative' in d['category']]
    if len(cumulative_files) >= 2:
        # æ’åºï¼šå¹´ä»½å¤§çš„åœ¨å‰
        cumulative_files.sort(key=lambda x: x['year'], reverse=True)
        
        df_cur = cumulative_files[0]['df']
        d_cur = cumulative_files[0]['date_str']
        
        df_lst = cumulative_files[1]['df']
        d_lst = cumulative_files[1]['date_str']
    
    # æª¢æŸ¥æ˜¯å¦éƒ½æ‰¾åˆ°äº†
    if df_wk is None or df_cur is None or df_lst is None:
        print("âŒ è‡ªå‹•è¾¨è­˜å¤±æ•—ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆå…§å®¹æ—¥æœŸæ˜¯å¦æ­£ç¢ºã€‚")
        print(f"è¾¨è­˜çµæœ: {[d['category'] for d in file_data_map.values()]}")
        return

    print(f"âœ… æˆåŠŸè¾¨è­˜ï¼š\n   æœ¬æœŸ: {d_wk}\n   ä»Šå¹´: {d_cur}\n   å»å¹´: {d_lst}")

    # --- 3. è³‡æ–™æ¸…ç†èˆ‡è¨ˆç®—å‡½å¼ ---
    def process_data(df_raw):
        # æŠ“å–è³‡æ–™åˆ—
        df_data = df_raw[df_raw[0].notna()].copy()
        df_data = df_data[df_data[0].str.contains("ç¸½è¨ˆ|æ´¾å‡ºæ‰€")].copy()
        df_data = df_data.reset_index(drop=True)
        
        columns_map = {
            0: "Station",
            1: "Total_Cases", 2: "Total_Deaths", 3: "Total_Injuries",
            4: "A1_Cases", 5: "A1_Deaths", 6: "A1_Injuries",
            7: "A2_Cases", 8: "A2_Deaths", 9: "A2_Injuries",
            10: "A3_Cases"
        }
        df_data = df_data.rename(columns=columns_map)
        
        for c in list(columns_map.values()):
            if c not in df_data.columns: df_data[c] = 0
        df_data = df_data[list(columns_map.values())]
        
        for col in list(columns_map.values())[1:]:
            df_data[col] = pd.to_numeric(df_data[col].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
            
        df_data['Station_Short'] = df_data['Station'].str.replace('æ´¾å‡ºæ‰€', 'æ‰€').str.replace('ç¸½è¨ˆ', 'åˆè¨ˆ')
        
        # é‡æ–°è¨ˆç®—åˆè¨ˆ (ç¢ºä¿è³‡æ–™æ­£ç¢º)
        df_stations = df_data[~df_data['Station_Short'].str.contains("åˆè¨ˆ")].copy()
        numeric_cols = df_data.columns[1:-1]
        total_row = df_stations[numeric_cols].sum()
        total_row['Station_Short'] = 'åˆè¨ˆ'
        df_total = pd.DataFrame([total_row])
        
        return pd.concat([df_total, df_stations], ignore_index=True)

    df_wk_clean = process_data(df_wk)
    df_cur_clean = process_data(df_cur)
    df_lst_clean = process_data(df_lst)

    # 4. æº–å‚™æ¨™é¡Œæ—¥æœŸ
    def format_date(s):
        m = re.findall(r'/(\d{2})/(\d{2})', s)
        return f"{m[0][0]}{m[0][1]}~{m[1][0]}{m[1][1]}" if len(m)>=2 else s

    h_wk = format_date(d_wk)
    h_cur = format_date(d_cur)
    h_lst = format_date(d_lst)

    # 5. åˆä½µèˆ‡è¨ˆç®— (å¼·åˆ¶æ­£ç¢ºå°æ‡‰)
    
    # --- A1 ---
    # ç¢ºä¿æ¬„ä½åç¨±å”¯ä¸€
    a1_wk = df_wk_clean[['Station_Short', 'A1_Deaths']].rename(columns={'A1_Deaths': 'wk'})
    a1_cur = df_cur_clean[['Station_Short', 'A1_Deaths']].rename(columns={'A1_Deaths': 'cur'})
    a1_lst = df_lst_clean[['Station_Short', 'A1_Deaths']].rename(columns={'A1_Deaths': 'last'})
    
    m_a1 = pd.merge(a1_wk, a1_cur, on='Station_Short', how='outer')
    m_a1 = pd.merge(m_a1, a1_lst, on='Station_Short', how='outer').fillna(0)
    
    m_a1['Diff'] = m_a1['cur'] - m_a1['last'] # ä»Šå¹´ - å»å¹´

    # --- A2 ---
    a2_wk = df_wk_clean[['Station_Short', 'A2_Injuries']].rename(columns={'A2_Injuries': 'wk'})
    a2_cur = df_cur_clean[['Station_Short', 'A2_Injuries']].rename(columns={'A2_Injuries': 'cur'})
    a2_lst = df_lst_clean[['Station_Short', 'A2_Injuries']].rename(columns={'A2_Injuries': 'last'})
    
    m_a2 = pd.merge(a2_wk, a2_cur, on='Station_Short', how='outer')
    m_a2 = pd.merge(m_a2, a2_lst, on='Station_Short', how='outer').fillna(0)
    
    m_a2['Diff'] = m_a2['cur'] - m_a2['last'] # ä»Šå¹´ - å»å¹´
    m_a2['Pct'] = m_a2.apply(lambda x: (x['Diff']/x['last']) if x['last']!=0 else 0, axis=1)
    m_a2['Pct_Str'] = m_a2['Pct'].apply(lambda x: f"{x:.2%}") # è½‰æˆ % å­—ä¸²
    m_a2['Prev'] = "-"

    # 6. æ’åº
    target_order = ['åˆè¨ˆ', 'è–äº­æ‰€', 'é¾æ½­æ‰€', 'ä¸­èˆˆæ‰€', 'çŸ³é–€æ‰€', 'é«˜å¹³æ‰€', 'ä¸‰å’Œæ‰€']
    order_map = {name: i for i, name in enumerate(target_order)}
    
    m_a1['order'] = m_a1['Station_Short'].map(order_map).fillna(99)
    m_a1 = m_a1.sort_values('order').drop(columns=['order'])
    
    m_a2['order'] = m_a2['Station_Short'].map(order_map).fillna(99)
    m_a2 = m_a2.sort_values('order').drop(columns=['order'])

    # 7. æ•´ç†æœ€çµ‚è¡¨æ ¼
    a1_final = m_a1[['Station_Short', 'wk', 'cur', 'last', 'Diff']].copy()
    a1_final.columns = ['å–®ä½', f'æœ¬æœŸ({h_wk})', f'æœ¬å¹´ç´¯è¨ˆ({h_cur})', f'å»å¹´ç´¯è¨ˆ({h_lst})', 'æœ¬å¹´èˆ‡å»å¹´åŒæœŸæ¯”è¼ƒ']
    
    a2_final = m_a2[['Station_Short', 'wk', 'Prev', 'cur', 'last', 'Diff', 'Pct_Str']].copy()
    a2_final.columns = ['å–®ä½', f'æœ¬æœŸ({h_wk})', 'å‰æœŸ', f'æœ¬å¹´ç´¯è¨ˆ({h_cur})', f'å»å¹´ç´¯è¨ˆ({h_lst})', 'æœ¬å¹´èˆ‡å»å¹´åŒæœŸæ¯”è¼ƒ', 'æœ¬å¹´è¼ƒå»å¹´å¢æ¸›æ¯”ä¾‹']

    # --- GOOGLE SHEETS ä¸²æ¥ ---
    print("ğŸ” æ­£åœ¨é©—è­‰ Google å¸³è™Ÿæ¬Šé™ (è«‹åœ¨è·³å‡ºçš„è¦–çª—é»é¸ã€å…è¨±ã€)...")
    try:
        auth.authenticate_user()
        creds, _ = default()
        gc = gspread.authorize(creds)
    except Exception as e:
        print(f"âŒ é©—è­‰å¤±æ•—ï¼š{e}")
        return

    # å»ºç«‹æ–°è©¦ç®—è¡¨
    sheet_name = f'äº¤é€šäº‹æ•…çµ±è¨ˆè¡¨_æ•´ç†çµæœ_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    try:
        sh = gc.create(sheet_name)
    except Exception as e:
        print(f"âŒ å»ºç«‹è©¦ç®—è¡¨å¤±æ•—ï¼š{e}")
        return

    # å¯«å…¥ A1 åˆ†é 
    try:
        ws1 = sh.sheet1
        ws1.update_title("A1æ­»äº¡äººæ•¸")
        ws1.update([a1_final.columns.values.tolist()] + a1_final.fillna(0).values.tolist())
    except Exception as e:
        print(f"âš ï¸ å¯«å…¥ A1 åˆ†é æ™‚ç™¼ç”Ÿå°å•é¡Œï¼š{e}")

    # å¯«å…¥ A2 åˆ†é 
    try:
        ws2 = sh.add_worksheet(title="A2å—å‚·äººæ•¸", rows=20, cols=10)
        ws2.update([a2_final.columns.values.tolist()] + a2_final.fillna(0).values.tolist())
    except Exception as e:
        print(f"âš ï¸ å¯«å…¥ A2 åˆ†é æ™‚ç™¼ç”Ÿå°å•é¡Œï¼š{e}")

    print("\n" + "="*40)
    print(f"âœ… æˆåŠŸï¼å·²è‡ªå‹•è¾¨è­˜æª”æ¡ˆä¸¦ç”¢ç”Ÿå ±è¡¨ï¼š")
    print(f"ğŸ”— é€£çµï¼š{sh.url}")
    print("="*40)

if __name__ == "__main__":
    analyze_traffic_stats_gsheet_smart()
