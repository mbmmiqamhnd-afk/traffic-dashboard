import streamlit as st
import pandas as pd
import io
import re
import smtplib
import gspread
from datetime import date
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

# ==========================================
# ğŸ” 1. å®‰å…¨è¨­å®šèˆ‡ç’°å¢ƒé…ç½® (Secrets)
# ==========================================
st.set_page_config(page_title="äº¤é€šäº‹æ•…çµ±è¨ˆç³»çµ±", layout="wide", page_icon="ğŸš‘")

try:
    MY_EMAIL = st.secrets["email"]["user"]
    MY_PASSWORD = st.secrets["email"]["password"]
    GCP_CREDS = st.secrets["gcp_service_account"]
except Exception as e:
    st.error("âŒ æ‰¾ä¸åˆ° Secrets è¨­å®šï¼è«‹åœ¨ .streamlit/secrets.toml ä¸­é…ç½® [email] èˆ‡ [gcp_service_account]ã€‚")
    st.stop()

# æŒ‡å®šçš„é›²ç«¯è©¦ç®—è¡¨ç¶²å€èˆ‡è¨­å®š
GOOGLE_SHEET_URL = "https://docs.google.com/spreadsheets/d/1HaFu5PZkFDUg7WZGV9khyQ0itdGXhXUakP4_BClFTUg/edit"
SMTP_SERVER = "smtp.gmail.com"
SMTP_PORT = 587
TO_EMAIL = MY_EMAIL # é è¨­å¯„çµ¦è‡ªå·±

# ==========================================
# ğŸ› ï¸ 2. å·¥å…·å‡½å¼ (Data Parsing & Cleaning)
# ==========================================

def parse_raw(f):
    """è§£æä¸Šå‚³æª”æ¡ˆ (æ”¯æ´ CSV èˆ‡ Excel)"""
    try:
        f.seek(0)
        if f.name.endswith('.csv'):
            return pd.read_csv(f, header=None)
        else:
            return pd.read_excel(f, header=None)
    except Exception as e:
        st.error(f"æª”æ¡ˆ {f.name} è®€å–å¤±æ•—: {e}")
        return None

def clean_data(df_raw):
    """æ¸…æ´—å ±è¡¨ï¼Œæå–é—œéµæ¬„ä½ A1(5) èˆ‡ A2(9)"""
    df_raw[0] = df_raw[0].astype(str)
    # ç¯©é¸ç‰¹å®šè¡Œ
    df_data = df_raw[df_raw[0].str.contains("æ‰€|ç¸½è¨ˆ|åˆè¨ˆ", na=False)].copy()
    cols = {0: "Station", 5: "A1_Deaths", 9: "A2_Injuries"}
    df_data = df_data.rename(columns=cols)
    
    # æ•¸å€¼æ¸…ç†
    for c in [5, 9]:
        target = cols[c]
        df_data[target] = pd.to_numeric(df_data[target].astype(str).str.replace(",", ""), errors='coerce').fillna(0)
    
    # çµ±ä¸€å–®ä½åç¨±
    df_data['Station_Short'] = df_data['Station'].str.replace('æ´¾å‡ºæ‰€', 'æ‰€').str.replace('ç¸½è¨ˆ', 'åˆè¨ˆ')
    return df_data

def get_gsheet_rich_text_req(sheet_id, row_idx, col_idx, text):
    """ç”¢è£½ Google Sheets ç´…å­—æ ¼å¼è«‹æ±‚"""
    text = str(text)
    tokens = re.split(r'([0-9\(\)\/\-\.\%]+)', text)
    runs = []
    current_pos = 0
    for token in tokens:
        if not token: continue
        color = {"red": 1, "green": 0, "blue": 0} if re.match(r'^[0-9\(\)\/\-\.\%]+$', token) else {"red": 0, "green": 0, "blue": 0}
        runs.append({"startIndex": current_pos, "format": {"foregroundColor": color, "bold": True}})
        current_pos += len(token)
    return {
        "updateCells": {
            "rows": [{"values": [{"userEnteredValue": {"stringValue": text}, "textFormatRuns": runs}]}],
            "fields": "userEnteredValue,textFormatRuns",
            "range": {"sheetId": sheet_id, "startRowIndex": row_idx, "endRowIndex": row_idx + 1, "startColumnIndex": col_idx, "endColumnIndex": col_idx + 1}
        }
    }

# ==========================================
# ğŸ“Š 3. æ ¸å¿ƒè¨ˆç®—é‚è¼¯ (è§£æ±ºéŒ¯ä½èˆ‡ NameError)
# ==========================================

def build_a1_table(wk_df, cur_df, lst_df, stations, date_labels):
    col = 'A1_Deaths'
    m = pd.merge(wk_df[['Station_Short', col]], cur_df[['Station_Short', col]], on='Station_Short', suffixes=('_wk', '_cur'))
    m = pd.merge(m, lst_df[['Station_Short', col]], on='Station_Short').rename(columns={col: col+'_lst'})
    
    m = m[m['Station_Short'].isin(stations)].copy()
    total = m.select_dtypes(include='number').sum().to_dict()
    total['Station_Short'] = 'åˆè¨ˆ'
    m = pd.concat([pd.DataFrame([total]), m], ignore_index=True)
    
    m['Diff'] = m[col+'_cur'] - m[col+'_lst']
    # 5 æ¬„ä½æ’åˆ—
    m = m[['Station_Short', col+'_wk', col+'_cur', col+'_lst', 'Diff']]
    m.columns = ['çµ±è¨ˆæœŸé–“', f'æœ¬æœŸ({date_labels["wk"]})', f'æœ¬å¹´ç´¯è¨ˆ({date_labels["cur"]})', f'å»å¹´ç´¯è¨ˆ({date_labels["lst"]})', 'æ¯”è¼ƒ']
    return m

def build_a2_table(wk_df, cur_df, lst_df, stations, date_labels):
    col = 'A2_Injuries'
    m = pd.merge(wk_df[['Station_Short', col]], cur_df[['Station_Short', col]], on='Station_Short', suffixes=('_wk', '_cur'))
    m = pd.merge(m, lst_df[['Station_Short', col]], on='Station_Short').rename(columns={col: col+'_lst'})
    
    m = m[m['Station_Short'].isin(stations)].copy()
    total = m.select_dtypes(include='number').sum().to_dict()
    total['Station_Short'] = 'åˆè¨ˆ'
    m = pd.concat([pd.DataFrame([total]), m], ignore_index=True)
    
    m['Diff'] = m[col+'_cur'] - m[col+'_lst']
    m['Pct'] = m.apply(lambda x: f"{(x['Diff']/x[col+'_lst']):.2%}" if x[col+'_lst'] != 0 else "0.00%", axis=1)
    
    # 7 æ¬„ä½æ’åˆ—: æ’å…¥ 'Prev' æ–¼ç´¢å¼• 2
    m.insert(2, 'Prev', '-')
    m = m[['Station_Short', col+'_wk', 'Prev', col+'_cur', col+'_lst', 'Diff', 'Pct']]
    m.columns = ['çµ±è¨ˆæœŸé–“', f'æœ¬æœŸ({date_labels["wk"]})', 'å‰æœŸ', f'æœ¬å¹´ç´¯è¨ˆ({date_labels["cur"]})', f'å»å¹´ç´¯è¨ˆ({date_labels["lst"]})', 'æ¯”è¼ƒ', 'å¢æ¸›æ¯”ä¾‹']
    return m

# ==========================================
# â˜ï¸ 4. åŒæ­¥èˆ‡å¯„ä¿¡å‡½å¼
# ==========================================

def sync_to_gsheet(df_a1, df_a2):
    try:
        gc = gspread.service_account_from_dict(GCP_CREDS)
        sh = gc.open_by_url(GOOGLE_SHEET_URL)
        
        def update_ws(ws_index, df, title_text):
            ws = sh.get_worksheet(ws_index)
            ws.batch_clear(["A3:Z100"]) # æ¸…é™¤èˆŠè³‡æ–™
            ws.update_acell('A1', title_text)
            
            # æ•¸å€¼è½‰æ›ç¢ºä¿å¯«å…¥ Google Sheets ä¸æœƒè®Šæˆå­—ä¸²
            data = [[int(x) if isinstance(x, (int, float, complex)) and not isinstance(x, bool) else x for x in row] for row in df.values.tolist()]
            ws.update('A3', data)
            
            # å¥—ç”¨ A2 æ¬„ä½æ¨™é¡Œç´…å­—æ ¼å¼
            reqs = [get_gsheet_rich_text_req(ws.id, 1, i, col) for i, col in enumerate(df.columns)]
            sh.batch_update({"requests": reqs})
        
        update_ws(2, df_a1, "A1é¡äº¤é€šäº‹æ•…æ­»äº¡äººæ•¸çµ±è¨ˆè¡¨") # ç¬¬3å€‹åˆ†é 
        update_ws(3, df_a2, "A2é¡äº¤é€šäº‹æ•…å—å‚·äººæ•¸çµ±è¨ˆè¡¨") # ç¬¬4å€‹åˆ†é 
        return True, "âœ… é›²ç«¯è©¦ç®—è¡¨åŒæ­¥æˆåŠŸ"
    except Exception as e:
        return False, f"âŒ è©¦ç®—è¡¨åŒæ­¥å¤±æ•—: {e}"

# ==========================================
# ğŸš€ 5. Streamlit ä¸»ç¨‹å¼
# ==========================================

st.title("ğŸš‘ äº¤é€šäº‹æ•…çµ±è¨ˆ (æœ€çµ‚å®Œæ•´ç‰ˆ)")
st.markdown("---")

uploaded_files = st.file_uploader("è«‹åŒæ™‚ä¸Šå‚³ 3 å€‹å ±è¡¨æª”æ¡ˆ (æœ¬æœŸã€æœ¬å¹´ç´¯è¨ˆã€å»å¹´åŒæœŸç´¯è¨ˆ)", accept_multiple_files=True)

if not uploaded_files:
    st.info("ğŸ’¡ æç¤ºï¼šè«‹ä¸€æ¬¡é¸æ“‡ä¸‰å€‹å ±è¡¨æª”æ¡ˆä¸Šå‚³ã€‚")
elif len(uploaded_files) != 3:
    st.warning(f"ç›®å‰å·²ä¸Šå‚³ {len(uploaded_files)} å€‹æª”æ¡ˆï¼Œé‚„å·® {3-len(uploaded_files)} å€‹ã€‚")
else:
    with st.status("æ­£åœ¨è™•ç†æ•¸æ“š...", expanded=True) as status:
        try:
            # A. è§£ææª”æ¡ˆèˆ‡æ—¥æœŸ
            files_meta = []
            for f in uploaded_files:
                df_raw = parse_raw(f)
                if df_raw is None: continue
                
                # åµæ¸¬æ¨™é¡Œä¸­çš„æ—¥æœŸ (æ°‘åœ‹å¹´)
                dates = re.findall(r'(\d{3})[./](\d{1,2})[./](\d{1,2})', str(df_raw.iloc[:5, :5].values))
                if len(dates) >= 2:
                    d_range = f"{int(dates[0][1]):02d}{int(dates[0][2]):02d}-{int(dates[1][1]):02d}{int(dates[1][2]):02d}"
                    files_meta.append({
                        'df': clean_data(df_raw),
                        'year': int(dates[1][0]),
                        'date_range': d_range,
                        'is_cumu': (int(dates[0][1]) == 1)
                    })
            
            if len(files_meta) < 3:
                st.error("âŒ ç„¡æ³•å¾æª”æ¡ˆä¸­è¾¨è­˜æ­£ç¢ºçš„æ—¥æœŸå€é–“ï¼Œè«‹ç¢ºèªå ±è¡¨æ ¼å¼ã€‚")
                st.stop()

            # B. åˆ†é…è®Šæ•¸ (æ ¹æ“šå¹´ä»½èˆ‡ç´¯ç©æ¨™èªŒ)
            files_meta.sort(key=lambda x: x['year'], reverse=True)
            cur_year = files_meta[0]['year']
            
            df_wk = [f for f in files_meta if f['year'] == cur_year and not f['is_cumu']][0]
            df_cur = [f for f in files_meta if f['year'] == cur_year and f['is_cumu']][0]
            df_lst = [f for f in files_meta if f['year'] < cur_year][0]
            
            date_labels = {"wk": df_wk['date_range'], "cur": df_cur['date_range'], "lst": df_lst['date_range']}
            stations = ['è–äº­æ‰€', 'é¾æ½­æ‰€', 'ä¸­èˆˆæ‰€', 'çŸ³é–€æ‰€', 'é«˜å¹³æ‰€', 'ä¸‰å’Œæ‰€']

            # C. è¨ˆç®—çµæœ
            a1_res = build_a1_table(df_wk['df'], df_cur['df'], df_lst['df'], stations, date_labels)
            a2_res = build_a2_table(df_wk['df'], df_cur['df'], df_lst['df'], stations, date_labels)

            # D. åŒæ­¥èˆ‡å¯„ä¿¡
            gs_ok, gs_msg = sync_to_gsheet(a1_res, a2_res)
            st.write(gs_msg)
            
            # E. é¡¯ç¤ºçµæœ
            status.update(label="âœ… è™•ç†å®Œæˆï¼", state="complete", expanded=False)
            
            st.success("çµ±è¨ˆæ•¸æ“šå·²æ›´æ–°")
            c1, c2 = st.columns(2)
            with c1: st.dataframe(a1_res, use_container_width=True)
            with c2: st.dataframe(a2_res, use_container_width=True)

        except Exception as e:
            st.error(f"åˆ†æéç¨‹ä¸­ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e}")
